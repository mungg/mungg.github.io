<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yekyung Kim</title>
  
  <meta name="author" content="Yekyung Kim">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="images/grogu_pixel.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yekyung Kim</name>
              </p>
              <p>I am a senior research engineer at Hyundai Motors Group, working on natural language processing, machine learning and its application for conversational AI.
              </p>
              <p>Before joining the hyundai, I started my industry journey at LG Electronics as a research engineer. I was fortunate to be selected as a <a href=http://www.koreatimes.co.kr/www/tech/2021/02/133_267589.html">specialist in AI</a> (Top 12 out of research employees) and researched at <a href="https://lti.cs.cmu.edu/">CMU LTI</a> as a visiting scientist mentored by <a href="https://www.cs.cmu.edu/~jgc/">Jaime Carbonell</a>.
              </p>
              <p>In 2015, I received my master's degree with outstanding paper award from Seoul National University, where I was in <a href="http://marg.snu.ac.kr"> Music and Audio Research Group (MARG)</a> advised by Kyogu Lee and Bongwon Suh.
              </p>
<!--              <p>-->
<!--                At Hyundai and LG Electronics I've worked on <a href="https://www.google.com/glass/start/">car AI assistant</a>,  <a href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html"></a>, <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://blog.google/products/google-ar-vr/introducing-next-generation-jump/">Jump</a>, <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, <a href="https://ai.googleblog.com/2020/12/portrait-light-enhancing-portrait.html">Portrait Light</a>, and <a href="https://www.matthewtancik.com/nerf">NeRF</a>. I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a> and funded by the <a href="http://www.nsfgrfp.org/">NSF GRFP</a>. I've received the <a href="https://www2.eecs.berkeley.edu/Students/Awards/15/">C.V. Ramamoorthy Distinguished Research Award</a> and the <a href="https://www.thecvf.com/?page_id=413#YRA">PAMI Young Researcher Award</a>.-->
<!--              </p>-->
              <p style="text-align:center">
                <a href="mailto:sigma89kim@gmail.com">Email</a> &nbsp/&nbsp
                <a href="TODO">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.co.kr/citations?user=6Rn81SsAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/mungg">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/yekyung_crop.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/yekyung_crop.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research goal is to build an efficient and trustworthy system that connects humans and machines leveraging language as a bridge. From learning machine knowledge with limited
resources to expanding machine knowledge to have human-like abilities, I am interested
in the reliable communication between humans and machines and how they can learn and
teach knowledge with as little supervision as possible.
<!--                I'm interested in computer vision, machine learning, optimization, and image processing. Much of my research is about inferring the physical world (shape, motion, color, light, etc) from images. Representative papers are <span class="highlight">highlighted</span>.-->
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/LINDA_profile2.png" width="160"></td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href=TODO">
                <papertitle>LINDA: Unsupervised Learning to Interpolate in Natural Language Processing</papertitle>
              </a>
              <br>
              <strong>Yekyung Kim</strong>,
              Seohyeong Jeong,
              <a href="https://kyunghyuncho.me/">Kyunghyun Cho</a>
              <br>
              <em>Under Review at Journal of Machine Learning Research </em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2112.13969.pdf">arXiv</a>
              /
              <a href="https://github.com/mungg/LINDA">code</a>
              <p></p>
              <p>
                We propose an unsupervised learning approach to text interpolation for the purpose of data augmentation.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/InfoVerse_profile.png" width="160"></td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href=TODO">
                <papertitle>infoVerse: Dataset Characterization with Multi-dimensional Meta-information</papertitle>
              </a>
              <br>
              <a href="https://sites.google.com/view/jaehyungkim">Jaehyung Kim</a>
              <strong>Yekyung Kim</strong>,
              <a href="https://dykang.github.io/">Dongyeop Kang</a>
              <br>
              <em>Arxiv</em>, 2022
              <br>
              <a href="TODO">arXiv</a>
              /
              <a href="TODO">code</a>
              <p></p>
              <p>
                We propose a data-centric framework to construct a new feature space that can capture various characteristics of datasets and novel sampling method to select a set of data points that maximizes the group informativeness.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/metacraft_profile.png" width="160"></td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href=TODO">
                <papertitle>Meta-Crafting: Improved Detection of Out-of-distributed Texts via Crafting Metadata Space</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/kooryan/">Ryan Koo</a>,
              <strong>Yekyung Kim</strong>,
              <a href="https://dykang.github.io/">Dongyeop Kang</a>,
              <a href="https://sites.google.com/view/jaehyungkim">Jaehyung Kim</a>
              <br>
              <em>Under Review at European Chapter of the Association for Computational Linguistics  </em>, 2022
              <br>
              <a href="TODO">arXiv</a>
              /
              <a href="TODO">code</a>
              <p></p>
              <p>
                We propose Meta-Crafting: a unified method that can capture both background and semantic shifts.  Specifically, we construct a new discriminative feature space to detect OOD samples
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/Active_profile.png" width="160"></td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href=TODO">
                <papertitle>Deep Active Learning for Sequence Labeling Based on Diversity and Uncertainty in Gradient</papertitle>
              </a>
              <br>
              <strong>Yekyung Kim</strong>
              <br>
              <em>Workshop on Life-long Learning for Spoken Language Systems at AACL</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2011.13570.pdf">arXiv</a>
              /
              <a href="TODO">code</a>
              <p></p>
              <p>
                I demonstrate that the amount of labeled training data can be reduced using active learning when it incorporates both uncertainty and diversity in the sequence labeling task.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/Korean_profile.png" width="160"></td>
            <td style="padding:20px;width:75%;vertical-align:middle">

                <papertitle>Learning Sub-Character level representation forKorean Named Entity Recognition</papertitle>

              <br>
              <a href="https://www.linkedin.com/in/yejin-jenny-kim/">Yejin Kim</a>,
              <strong>Yekyung Kim</strong> (equal contributions)
              <br>
              <em>The International FLAIRS Conference Proceedings </em>, 2020
              <br>
              <a href="https://journals.flvc.org/FLAIRS/article/view/128509">paper</a>
              <p></p>
              <p>
                We propose a improved unigram-level Korean NER model with sub-character level representation, jamo, which can represent a unique linguistic structure of Korean and its syntactic properties and morphological variations.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/Music_Twitter_profile.png" width="160"></td>
            <td style="padding:20px;width:75%;vertical-align:middle">

                <papertitle>#Nowplaying the Future Billboard: Mining Music Listening Behaviors of Twitter Users for Hit Song Prediction</papertitle>

              <br>
              <strong>Yekyung Kim</strong>,
              <a href="">Bongwon Suh</a>,
              <a href="">Kyogu Lee</a>
              <br>
              <em>Workshop on Social Media Retrieval and Analysis (SoMeRA) at SIGIR </em>, 2014
              <br>
              <a href="https://dl.acm.org/doi/abs/10.1145/2632188.2632206">paper</a>
              <p></p>
              <p>
                We collect users' music listening behavior from Twitter using music-related hashtags (eg,# nowplaying). We then build a predictive model to forecast the Billboard rankings and hit music.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/Adobe_Twitter_profile.png" width="160"></td>
            <td style="padding:20px;width:75%;vertical-align:middle">

                <papertitle>A Visual Analytics Approach to Summarizing Tweets</papertitle>

              <br>
              Ramik Sadana,
              <strong>Yekyung Kim</strong> ,
              <a href="">Bongwon Suh</a>,
              Eunyee Koh
              <br>
              <em>Industry day at SIGIR </em>, 2014
              <br>
              <a href="http://david-hawking.net/SIRIP2014_Proceedings/SIRIP'14-A%20Visual%20Analytics%20Approach%20to%20Summarizing%20Tweets.pdf">paper</a>
              <p></p>
              <p>
               We build on key principles of visual analytics and describe an end-to-end, visual exploration system for tweets that both presents overall summaries and supports analysis of any variations that exists in the activity
              </p>
            </td>
          </tr>

      </td>
    </tr>
  </table>
</body>

</html>
